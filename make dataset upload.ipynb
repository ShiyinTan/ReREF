{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13da835e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset, load_metric, DatasetDict\n",
    "rouge = load_metric(\"rouge\", trust_remote_code=True)\n",
    "import textdistance\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, LEDForConditionalGeneration\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sys\n",
    "sys.path.append('./DMRST_Parser/')\n",
    "\n",
    "from model_depth import ParsingNet\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import copy\n",
    "import string\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ade589",
   "metadata": {},
   "source": [
    "## load original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c6d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_docs(docs):\n",
    "    all_docs = docs.split(\"|||||\")\n",
    "    new_docs = []\n",
    "    for i, doc in enumerate(all_docs):\n",
    "        doc = re.sub(r'\\s*\\n\\s*\\n', ' ', str(doc))\n",
    "        doc = doc.replace('  ', ' ')\n",
    "        \n",
    "        if doc.strip() == '' or len(doc) < 50: # if empty\n",
    "            continue\n",
    "        new_docs.append(doc)\n",
    "\n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d405e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = load_dataset('multi_news', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144f249",
   "metadata": {},
   "source": [
    "## PRIMER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f81d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMER_path = 'allenai/PRIMERA'\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRIMER_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69722242",
   "metadata": {},
   "source": [
    "## EDU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c2c62e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, input_sentences, batch_size):\n",
    "    LoopNeeded = int(np.ceil(len(input_sentences) / batch_size))\n",
    "\n",
    "    input_sentences = [tokenizer.tokenize(i, add_special_tokens=False) for i in input_sentences]\n",
    "    all_segmentation_pred = []\n",
    "    all_tree_parsing_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for loop in range(LoopNeeded):\n",
    "            StartPosition = loop * batch_size\n",
    "            EndPosition = (loop + 1) * batch_size\n",
    "            if EndPosition > len(input_sentences):\n",
    "                EndPosition = len(input_sentences)\n",
    "\n",
    "            input_sen_batch = input_sentences[StartPosition:EndPosition]\n",
    "            _, _, SPAN_batch, _, predict_EDU_breaks = model.TestingLoss(input_sen_batch, input_EDU_breaks=None, LabelIndex=None,\n",
    "                                                                        ParsingIndex=None, GenerateTree=True, use_pred_segmentation=True)\n",
    "            all_segmentation_pred.extend(predict_EDU_breaks)\n",
    "            all_tree_parsing_pred.extend(SPAN_batch)\n",
    "    return input_sentences, all_segmentation_pred, all_tree_parsing_pred\n",
    "\n",
    "\n",
    "def inference_only_EDU_break(model, tokenizer, input_sentences, batch_size):\n",
    "    LoopNeeded = int(np.ceil(len(input_sentences) / batch_size))\n",
    "\n",
    "    input_sentences = [tokenizer.tokenize(i, add_special_tokens=False) for i in input_sentences]\n",
    "    all_segmentation_pred = []\n",
    "    all_tree_parsing_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for loop in range(LoopNeeded):\n",
    "            StartPosition = loop * batch_size\n",
    "            EndPosition = (loop + 1) * batch_size\n",
    "            if EndPosition > len(input_sentences):\n",
    "                EndPosition = len(input_sentences)\n",
    "\n",
    "            input_sen_batch = input_sentences[StartPosition:EndPosition]\n",
    "            EncoderOutputs, Last_Hiddenstates, _, predict_EDU_breaks = model.encoder(input_sen_batch, None, is_test=True)\n",
    "\n",
    "            all_segmentation_pred.extend(predict_EDU_breaks)\n",
    "    return input_sentences, all_segmentation_pred\n",
    "\n",
    "\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    text = ''.join([' '+token.lstrip('▁') if token.startswith('▁') else token for token in tokens])\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_list_by_positions(lst, positions):\n",
    "    result = []\n",
    "    prev_pos = 0\n",
    "    \n",
    "    for pos in positions:\n",
    "        token_list = lst[prev_pos:pos+1]\n",
    "        result.append(tokens_to_string(token_list)) \n",
    "        prev_pos = pos+1\n",
    "    result = '||'.join(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eb80d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDU_device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5885f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_path = './DMRST_Parser/depth_mode/Savings/multi_all_checkpoint.torchsave'\n",
    "\n",
    "edu_parsing_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", use_fast=True)\n",
    "edu_parsing_model = AutoModel.from_pretrained(\"xlm-roberta-base\").to(device)\n",
    "\n",
    "model = ParsingNet(edu_parsing_model, bert_tokenizer=edu_parsing_tokenizer, device=EDU_device)\n",
    "\n",
    "model = model.to(EDU_device)\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "model.load_state_dict(state_dict) \n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.encoder = model.encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009bf84a",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb2426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures # 用于并行计算\n",
    "\n",
    "def process_data(sample, doc_i):\n",
    "    row_data_dict = {}\n",
    "    document = sample['document']\n",
    "    \n",
    "    # process docs to get doc list\n",
    "    docs_list = preprocess_docs(document)\n",
    "    if len(docs_list)==0: # empty docs, return empty dict, and fail Flag\n",
    "        return row_data_dict, True\n",
    "    # generate EDUs for each doc\n",
    "    input_sentences, all_segmentation_pred, all_tree_parsing_pred = inference(model, edu_parsing_tokenizer, docs_list, 1)\n",
    "    # process EDU results, and back to docs\n",
    "    all_docs_edus = []\n",
    "    for doc_sent, doc_seg in zip(input_sentences, all_segmentation_pred):\n",
    "        doc_edus_splits = split_list_by_positions(doc_sent, doc_seg)\n",
    "\n",
    "        all_docs_edus.append(doc_edus_splits)\n",
    "\n",
    "    docs_edus = ' ||||| '.join(all_docs_edus)\n",
    "    row_data_dict['document'] = docs_edus\n",
    "    row_data_dict['summary'] = sample['summary']\n",
    "    \n",
    "    row_data_dict['parsing'] = all_tree_parsing_pred\n",
    "    row_data_dict['id'] = doc_i\n",
    "    return row_data_dict, False\n",
    "\n",
    "def process_data_only_EDU_breaks(sample, doc_i):\n",
    "    row_data_dict = {}\n",
    "    document = sample['document']\n",
    "    \n",
    "    # process docs to get doc list\n",
    "    docs_list = preprocess_docs(document)\n",
    "    if len(docs_list)==0: # empty docs, return empty dict, and Flag to skip this sample\n",
    "        return row_data_dict, True\n",
    "    # generate EDUs for each doc\n",
    "#     input_sentences, all_segmentation_pred, all_tree_parsing_pred = inference(model, edu_parsing_tokenizer, docs_list, 1)\n",
    "    input_sentences, all_segmentation_pred = inference_only_EDU_break(model, edu_parsing_tokenizer, docs_list, 1)\n",
    "    # process EDU results, and back to docs\n",
    "    all_docs_edus = []\n",
    "    for doc_sent, doc_seg in zip(input_sentences, all_segmentation_pred):\n",
    "        doc_edus_splits = split_list_by_positions(doc_sent, doc_seg)\n",
    "\n",
    "        all_docs_edus.append(doc_edus_splits)\n",
    "\n",
    "    docs_edus = ' ||||| '.join(all_docs_edus)\n",
    "    row_data_dict['document'] = docs_edus\n",
    "    row_data_dict['summary'] = sample['summary']\n",
    "    \n",
    "#     row_data_dict['parsing'] = all_tree_parsing_pred\n",
    "    row_data_dict['id'] = doc_i\n",
    "    return row_data_dict, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fbb1762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 456/44972 [01:02<1:15:35,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 453, empty docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3731/44972 [07:57<1:15:21,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 3728, empty docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 16292/44972 [34:54<31:43, 15.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 16290, empty docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 16491/44972 [35:21<1:00:10,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 16489, empty docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 18811/44972 [40:10<1:20:14,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 18812, empty docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 19282/44972 [41:06<50:13,  8.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 19279, empty docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 21622/44972 [45:53<30:50, 12.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 21620, empty docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 30739/44972 [1:05:06<15:33, 15.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 30735, empty docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 41997/44972 [1:28:55<03:27, 14.32it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 41993, empty docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44972/44972 [1:35:16<00:00,  7.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. train data process\n",
    "trian_data_list = []\n",
    "\n",
    "for i in tqdm(range(original_dataset['train'].num_rows)):\n",
    "    sample = original_dataset['train'][i]\n",
    "    \n",
    "    row_data_dict, flag_to_skip = process_data_only_EDU_breaks(sample, i)\n",
    "    if flag_to_skip:\n",
    "        print(f\"doc id: {i}, empty docs\")\n",
    "        continue\n",
    "    trian_data_list.append(row_data_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03945836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 4855/5622 [10:03<01:08, 11.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 4850, empty docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5622/5622 [11:42<00:00,  8.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# 2. validation data process\n",
    "validation_data_list = []\n",
    "\n",
    "for i in tqdm(range(original_dataset['validation'].num_rows)):\n",
    "    sample = original_dataset['validation'][i]\n",
    "\n",
    "    row_data_dict, flag_to_skip = process_data_only_EDU_breaks(sample, i)\n",
    "    if flag_to_skip:\n",
    "        print(f\"doc id: {i}, empty docs\")\n",
    "        continue\n",
    "    validation_data_list.append(row_data_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a2b6587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 4738/5622 [10:18<01:35,  9.29it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 4736, empty docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5622/5622 [12:14<00:00,  7.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3. test data process\n",
    "test_data_list = []\n",
    "\n",
    "for i in tqdm(range(original_dataset['test'].num_rows)):\n",
    "    sample = original_dataset['test'][i]\n",
    "\n",
    "    row_data_dict, flag_to_skip = process_data_only_EDU_breaks(sample, i)\n",
    "    if flag_to_skip:\n",
    "        print(f\"doc id: {i}, empty docs\")\n",
    "        continue\n",
    "    test_data_list.append(row_data_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "71d29a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save processed dict dataset to json form, and save\n",
    "\n",
    "with open('dataset/my_processed_dataset/trian_data_list.json', 'w') as json_file:\n",
    "    json.dump(trian_data_list, json_file)\n",
    "\n",
    "with open('dataset/my_processed_dataset/validation_data_list.json', 'w') as json_file:\n",
    "    json.dump(validation_data_list, json_file)\n",
    "\n",
    "with open('dataset/my_processed_dataset/test_data_list.json', 'w') as json_file:\n",
    "    json.dump(test_data_list, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a70b8",
   "metadata": {},
   "source": [
    "## Load dataset and push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = load_dataset(\"json\", data_files={'train':\"dataset/my_processed_dataset/trian_data_list.json\",\n",
    "                                                  'validation':\"dataset/my_processed_dataset/validation_data_list.json\",\n",
    "                                                  'test':\"dataset/my_processed_dataset/test_data_list.json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ce46aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c17fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_dataset.push_to_hub(\"HF-Data-for-Retriever/multi_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955b8ff",
   "metadata": {},
   "source": [
    "## Make ground-truth of docs ranking and similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48d62f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# sent_model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
    "sent_model = SentenceTransformer('multi-qa-mpnet-base-cos-v1', device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee9367",
   "metadata": {},
   "source": [
    "### Get dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7fb7ed3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_dataset = load_dataset('HF-Data-for-Retriever/multi_news')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918d43e",
   "metadata": {},
   "source": [
    "### Calculate doc similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "268c0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meaningful_words(text):\n",
    "    punctuations = string.punctuation + \"“”‘’—–``'''s...\" # punctuations with additional punctuations\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    meaningful_words = [word for word in tokens if word.lower() not in stop_words and word.lower() not in punctuations]\n",
    "\n",
    "    return meaningful_words\n",
    "\n",
    "\n",
    "\n",
    "def chunk_text_with_slide(text, chunk_size=512, window_size=200):\n",
    "    assume_ratio_to_token_num = 1.5\n",
    "    words = text.split(' ')\n",
    "    chunk_word_num = int(chunk_size//assume_ratio_to_token_num)\n",
    "    window_word_size = int(window_size//assume_ratio_to_token_num)\n",
    "    text_chunks = []\n",
    "    chunk_assume_num = int((len(words)-chunk_word_num)//window_word_size + 1)\n",
    "    for i in range(chunk_assume_num): # window chunk 构建\n",
    "        chunk_start_id = i*window_word_size\n",
    "        text_chunks.append(\" \".join(words[chunk_start_id:chunk_start_id+chunk_word_num]))\n",
    "    text_chunks.append(\" \".join(words[-chunk_size:])) # 最后的文本作为chunk\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e25a3dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sent_transform_sim(seq_list, summary, model=None):\n",
    "    results = []\n",
    "    for seq in seq_list:\n",
    "        embedding1 = model.encode(seq, convert_to_tensor=True)\n",
    "        embedding2 = model.encode(summary, convert_to_tensor=True)\n",
    "\n",
    "        cosine_sim = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "        results.append(cosine_sim.item())\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d38f0",
   "metadata": {},
   "source": [
    "### Update dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4e859027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_new_column(sample):\n",
    "    sample['doc_len'] = len(sample['document'])\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "def add_new_column_doc_sim(sample): # rough_word_num\n",
    "    doc_list = sample['document'].split('|||||')\n",
    "    summary = sample['summary']\n",
    "    \n",
    "    doc_clean_list = []\n",
    "    doc_edus_list = []\n",
    "    doc_tokens_list = []\n",
    "    for i, doc in enumerate(doc_list):\n",
    "        doc = doc.lstrip()\n",
    "        edus = doc.split('||')\n",
    "        doc_edus_list.append(edus)\n",
    "        clean_doc = \"\".join(edus)\n",
    "        doc_clean_list.append(clean_doc)\n",
    "        doc_tokens_list.append(tokenizer.encode(clean_doc, add_special_tokens=False))\n",
    "    \n",
    "    sample['doc_token_num'] = [] # calculate the token number of each doc, need tokenize first, time consuming\n",
    "    sample['doc_rough_word_num'] = [] # calculate the rough word num of each doc, split by 'space'\n",
    "    for i in range(len(doc_clean_list)):\n",
    "        clean_doc = doc_clean_list[i]\n",
    "        doc_edus = doc_edus_list[i]\n",
    "        doc_tokens = doc_tokens_list[i]\n",
    "        rough_word_num = len(clean_doc.split(' '))\n",
    "        sample['doc_rough_word_num'].append(rough_word_num)\n",
    "        sample['doc_token_num'].append(len(doc_tokens))\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "def update_doc_sim(sample):\n",
    "    doc_list = sample['document'].split('|||||')\n",
    "    summary = sample['summary']\n",
    "    \n",
    "    doc_clean_list = []\n",
    "    doc_edus_list = []\n",
    "    for i, doc in enumerate(doc_list):\n",
    "        doc = doc.lstrip()\n",
    "        edus = doc.split('||')\n",
    "        doc_edus_list.append(edus)\n",
    "        clean_doc = \"\".join(edus)\n",
    "        doc_clean_list.append(clean_doc)\n",
    "    \n",
    "    \n",
    "    sample['sent_trans_doc_score'] = sent_transform_sim(doc_clean_list, summary, sent_model)\n",
    "    sample['sent_trans_doc_edu_score'] = [sent_transform_sim(doc_edus_list[i], summary, sent_model) for i in range(len(doc_edus_list))]\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dataset_new_train = my_dataset['train'].map(add_new_column_doc_sim)\n",
    "# my_dataset_new_validation = my_dataset['validation'].map(add_new_column_doc_sim)\n",
    "# my_dataset_new_test = my_dataset['test'].map(add_new_column_doc_sim)\n",
    "\n",
    "my_dataset_new_train = my_dataset['train'].map(update_doc_sim)\n",
    "my_dataset_new_validation = my_dataset['validation'].map(update_doc_sim)\n",
    "my_dataset_new_test = my_dataset['test'].map(update_doc_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c0611b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_dataset_new = DatasetDict({'train':my_dataset_new_train,\n",
    "                              'validation': my_dataset_new_validation, \n",
    "                              'test': my_dataset_new_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca7780",
   "metadata": {},
   "source": [
    "## login huggingface and push data to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ff4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login ## -> Need your own Huggingface token!\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ec7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset_new.push_to_hub(\"HF-Data-for-Retriever/multi_news\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mds_pre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
